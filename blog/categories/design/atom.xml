<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Design | Woodstock Blog]]></title>
  <link href="http://okc1.github.io/blog/categories/design/atom.xml" rel="self"/>
  <link href="http://okc1.github.io/"/>
  <updated>2016-07-11T01:30:05-07:00</updated>
  <id>http://okc1.github.io/</id>
  <author>
    <name><![CDATA[Kevin Durant]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[[Design] Design Twitter]]></title>
    <link href="http://okc1.github.io/blog/2016/07/11/design-twitter/"/>
    <updated>2016-07-11T00:00:00-07:00</updated>
    <id>http://okc1.github.io/blog/2016/07/11/design-twitter</id>
    <content type="html"><![CDATA[# System design evaluation form

1. work solution
1. special cases
1. analysis
1. trade off
1. knowledge base

# Design guideline: 4S

1. Scenario

    ask, features, qps, DAU, interfaces

1. Service

    split, application, module

1. Storage

    schema, data, sql, NoSql, file system

1. Scale

    sharding, optimize, special case

# Scenario

## DAU? 

Whta's the DAU/MAU rate?

Chatting apps like wechat/whatapp has a rate of around 75%, but facebook/twitter is lower at 60%.

## Enumerate the functions

1. registration
1. user profile display/edit
1. upload image/video
1. search
1. post a tweet
1. share a tweet
1. timeline
1. newsfeed
1. follow/unfollow

## QPS

1. concurrent user
    
    150M user * 60 query/user / 60*60*24s = average QPS = 100K
    
    __peak QPS = 3 * average QPS = 300K__
    
    fast growing product = 2 * peak QPS = 600K
    
1. read qps: 300K QPS
1. write qps: 5K QPS

On average, __a web server support around 1000 QPS__, thus in this case, we need 300 servers to support the system. 

# Service

## 4 services for Twitter

<img class="middle" src="/assets/images/jiuzhang-four-service.png">

1. user service
    1. register
    2. login
1. tweet service
    1. post tweet
    1. news feed
    1. timeline
1. media service
    1. upload image
    1. upload video
1. friendship service
    1. follow
    1. upfollow
    
# Storage

1. SQL
    
    __Good for__ accurate, small amount of data, more read than write. 
    user table
    
2. NoSQL

    __Good for__ large amount of read/write, high scalability. 
    tweets
    social graph (follower)
    
3. File System

    __Good for__ media files
    photo, video

## Select the right DB

<img class="middle" src="/assets/images/jiuzhang-db-selection.png">

__Question: can we use file system for tweets__?

No, it's hard to query. Eg. query all tweets of my friends. 

## Design data schema

(optional) 3 tables needed:

1. user table
2. tweet table
3. friendship table: this is not as straight forward, as it shall contain double directions info

<img class="middle" src="/assets/images/jiuzhang-data-schema.png">

# Important: News Feed

## pull model

Read top 50 feeds from top 100 friends, then merge sort by date. (note that user is getting sync-blocked here). 

Post tweet is simple 1 DB write.  

__This design is bad, because file-system/DB read is slow__. If you have N friends, you query O(N) DB queries. __It's too slow (and user is getting sync-blocked, too)__. We should have, ideally, <= 7 DB queries per web page.

<img class="middle" src="/assets/images/jiuzhang-pull-diagram.png">

<img class="middle" src="/assets/images/jiuzhang-pull-code.png">

### problem

1. synchronously block user from getting news feed
1. too many DB reads

## push model

Each person have a list storing new feeds. When friend post tweet, __fanout__ to my feed list. 

When I read, I simply read top 100 from the feed list. __So read is 1 DB read__. 

Post tweet is N DB writes, which is slow. __However this is done async, so it does not matter__.  

<img class="middle" src="/assets/images/jiuzhang-push-diagram.png">

<img class="middle" src="/assets/images/jiuzhang-push-code.png">

One example of async implementation: __RabbitMQ__

# Scale

## optimize pull model

Although it looks like push is faster than pull, __facebook and twitter both use pull model__.

1. add cache for DB, reduce # of DB read
2. also cache each user's news feed

    your yesterday's feeds are all cached, thus don't need to read everytime.

## optimize push model

1. disk waste a lot, although disk is cheap
1. inactive user! 

    rank follower by weight, and don't write to inactive user (eg. last login time)

1. if follower is toooo much, like Lady Gaga, user pull for Lady Gaga. 

    Tradeoff: Push + Pull model. 
    
## optimize 'Like' info

In tweet table, if we need to count(user) who liked, it's gonna take forever. 

__We must denormalize this data__!

<img class="middle" src="/assets/images/jiuzhang-denormalize.png">

Denormalize: it's duplicate info but we store in table, because of performance improvement. 

__Shortcoming: inconsistency__! 

1. unless using SQL transaction, async failure can result in wrong counting number
2. race condition

Solution: 1. use atomic operation 2. every day, schedule to update this number.

## optimize thundering hert problem

When cache fails, all DB query will go to DB. This results in DB crash.

Hot spot (thundering hert)

<img class="middle" src="/assets/images/jiuzhang-thundering-herd.png">

Solution: hold all incoming queries (who fails cache), and only send 1 DB query. When result is returned, return to every query. 
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Design] User Registry Table Design]]></title>
    <link href="http://okc1.github.io/blog/2016/05/08/user-registry-table-design/"/>
    <updated>2016-05-08T00:00:00-07:00</updated>
    <id>http://okc1.github.io/blog/2016/05/08/user-registry-table-design</id>
    <content type="html"><![CDATA[# First word 

Designing a system like twitter, facebook or airbnb, first step is often __User Registry__.

The tables, __we must use RDBMS__, as it's more reliable. 

## Table design

__Friendship table__ is important:

<img class="middle" src="/assets/images/design-user-registry-tables.png">
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Design] Technical Challenges of Writing a Web Crawler]]></title>
    <link href="http://okc1.github.io/blog/2015/11/22/challenges-writing-crawler/"/>
    <updated>2015-11-22T00:00:00-08:00</updated>
    <id>http://okc1.github.io/blog/2015/11/22/challenges-writing-crawler</id>
    <content type="html"><![CDATA[# 1. Choose a framework

Assuming we use Python to do this.

## plain python?

We can write a simple Python crawler with the code below:

    import re, urllib

    textfile = file('depth_1.txt','wt')
    print "Enter the URL you wish to crawl.."
    print 'Usage  - "http://phocks.org/stumble/creepy/" <-- With the double quotes'
    myurl = input("@> ")
    for i in re.findall('''href=["'](.[^"']+)["']''', urllib.urlopen(myurl).read(), re.I):
        print i  
        for ee in re.findall('''href=["'](.[^"']+)["']''', urllib.urlopen(i).read(), re.I):
            print ee
            textfile.write(ee+'\n')
    textfile.close()

## Scrapy?

1. You only define the rules, Scrapy do the rest
1. easily plugin extensions
1. portable + python runtime.

### Why Scrapy

> [scrapy has the tools to manage every stage of a web crawl](https://www.quora.com/What-are-the-advantages-of-Scrapy-compared-to-Beautiful-Soup), just to name a few:

> 1. Requests manager - in charge of downloading pages all concurrently behind the scenes! You won't need to invest a lot of time in concurrent architecture.
>
> 2. Selectors -  parse the html document (eg. XPath) 
>
> 3. Pipelines - after you retrieve the data, there's a bunch of functions to modify the data.

Following the spirit of other donâ€™t repeat yourself frameworks, such as Django:

> [it makes it easier to build and scale large crawling projects](https://en.wikipedia.org/wiki/Scrapy) by allowing developers to re-use their code. 

For more, read [Scrapy Architecture ](http://doc.scrapy.org/en/latest/topics/architecture.html).

<img class="middle" src="/assets/images/scrapy_architecture.png">

1. Scrapy Engine 

    control data flow

1. Scheduler 

    receives requests from the engine and enqueues them for feeding them later

1. Downloader

1. Spiders

1. Item Pipeline

1. Downloader middlewares

    specific hooks that sit between the Engine and the Downloader and process requests

1. Spider middlewares

    specific hooks that sit between the Engine and the Spiders and are able to process spider input (responses) and output (items and requests).

# 2. Schedule a Scrapy job

APScheduler? (todo)

add/remove jobs

# 3. Choose a DB

I chose NoSQL/MongoDB. [But why](http://stackoverflow.com/a/11980154)?

1. there's only a few tables with few columns

1. no overly complex associations between nodes

1. huge amount of time-based data

1. scaling requirements: MongoDB better horizontal scaling

1. different field names: dynamical storage

# 4. Technical Difficulty?

## 4.1 differrent way to crawl. 

We need to check AJAX response sometime and study each website's API. 

Some site would __close certain APIs__ if they found out too many queries requests. 

## 4.2 Difficulty navigating pages

Study their URL structure.

eg. 

    www.abc.com/index.html?page=milk&start_index=0
    
Just play with the url params!

## 4.3 What is key?

I defined extra column only to store keys (combine a few key columns, and convert to lower-case). 

We can search using __regex__ though, but:

> [Mongo (current version 2.0.0) doesn't allow](http://stackoverflow.com/a/7880894) case-insensitive searches against indexed fields. For non-indexed fields, the regex search should be fine.

How to go about it: 

> [searching with regex's case insensitive](http://stackoverflow.com/a/4441412) means that mongodb cannot search by index, so queries against __large datasets can take a long time__.

> Even with small datasets, it's not very efficient... which could become an issue if you are trying to achieve scale.

> As an alternative, you can store an uppercase copy and search against that... 

> If your field is large, such as a message body, duplicating data is probably not a good option. I believe using __an extraneous indexer like Apache Lucene__ is the best option in that case.

## 4.4 A lot bad data

1. write a sophisticated pipeline()

1. try not let bad data reach pipeline() - __better__

Make your spider better!

## 4.5 NLP: brand names

how? (todo)

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Design] How to Generate Maze]]></title>
    <link href="http://okc1.github.io/blog/2015/11/21/generate-maze/"/>
    <updated>2015-11-21T00:00:00-08:00</updated>
    <id>http://okc1.github.io/blog/2015/11/21/generate-maze</id>
    <content type="html"><![CDATA[# Question

[link](http://www.glassdoor.com/Interview/Design-a-2D-dungeon-crawling-game-It-must-allow-for-various-items-in-the-maze-walls-objects-and-computer-controlled-c-QTN_57.htm)

> Design a 2D dungeon crawling game. It must allow for   various items in the maze - walls, objects, and computer-controlled characters.

# Part 1: API design

Serialize:

> [if a certain cell has a wall](http://qr.ae/RbRhHv) to the North and West but not to the South or East, it would be represented as 1001, or 9... (e.g., "9,6,11,12\n3,10,10,4\n13,9,12,5\n3,6,1,6" in a 4x4 maze)

Design API~

# Part 2: Algorithm

## Depth-first search 

This is most common and [one of the simplest ways to generate a maze using a computer](https://en.wikipedia.org/wiki/Maze_generation_algorithm#Depth-first_search). It's commonly implemented using __[Recursive backtrack](https://en.wikipedia.org/wiki/Maze_generation_algorithm#Recursive_backtracker)__. 

1. from a random cell, select a random neighbour that hasn't been visited. 

1. removes the 'wall' and adds the new cell to a stack. 

1. a cell with no unvisited neighbours is considered __dead-end__. 

1. When at a dead-end it backtracks through the path until it reaches a cell with unvisited neighbours, continuing from there. 

1. until every cell has been visited, the computer would backtrack all the way to the beginning cell. 

1. Entire maze space is guaranted a complete visit.

### side note

> To add difficulty and a fun factor to the DFS, you can __influence the likelihood of which neighbor you should visit__, instead of completely random. 

> By making it more likely to visit neighbors to your sides, you can have a more horizontal maze generation.  
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Design] Strategy Design Pattern]]></title>
    <link href="http://okc1.github.io/blog/2015/11/18/strategy-pattern/"/>
    <updated>2015-11-18T00:00:00-08:00</updated>
    <id>http://okc1.github.io/blog/2015/11/18/strategy-pattern</id>
    <content type="html"><![CDATA[# Overview

__[Strategy pattern](https://en.wikipedia.org/wiki/Strategy_pattern)__ (also known as the policy pattern) is a design pattern that __enables an algorithm's behavior to be selected__ at runtime.

For instance, a class that performs __validation on incoming data__ may use a strategy pattern to select a validation algorithm __based on the type of data__, the source of the data, user choice... These factors are not known __until run-time__... 

## A car example

Since accelerate and brake behaviors change frequently between models, __a common approach is to implement these behaviors in subclasses__. This approach has significant drawbacks: accelerate and brake behaviors __must be declared in each new Car model__. 

<img class="middle" src="/assets/images/600px-StrategyPattern_IBrakeBehavior.png">

The strategy pattern uses __composition__ instead of inheritance. This allows: 

1. better __decoupling between the behavior__ and the class that uses it. (i.e. behavior can be changed without breaking the classes that use it)

1. classes can switch between behaviors by changing the specific implementation used without requiring any significant code changes. 

Code:

    /* Encapsulated family of Algorithms 
     * Interface and its implementations
     */
    public interface IBrakeBehavior {
        public void brake(); 
    }

    public class BrakeWithABS implements IBrakeBehavior {
        public void brake() {
            System.out.println("Brake with ABS applied");
        }
    }

    public class Brake implements IBrakeBehavior {
        public void brake() {
            System.out.println("Simple Brake applied");
        }
    }


    /* Client which can use the algorithms above interchangeably */
    public abstract class Car {
        protected IBrakeBehavior brakeBehavior;

        public void applyBrake() {
            brakeBehavior.brake();
        }

        public void setBrakeBehavior(IBrakeBehavior brakeType) {
            this.brakeBehavior = brakeType;
        }
    }

    /* Client 1 uses one algorithm (Brake) in the constructor */
    public class Sedan extends Car {
        public Sedan() {
            this.brakeBehavior = new Brake();
        }
    }

    /* Client 2 uses another algorithm (BrakeWithABS) in the constructor */
    public class SUV extends Car {
        public SUV() {
            this.brakeBehavior = new BrakeWithABS();
        }
    }


    /* Using the Car Example */
    public class CarExample {
        public static void main(String[] args) {
            Car sedanCar = new Sedan();
            sedanCar.applyBrake();  // This will invoke class "Brake"

            Car suvCar = new SUV(); 
            suvCar.applyBrake();    // This will invoke class "BrakeWithABS"

            // set brake behavior dynamically
            suvCar.setBrakeBehavior( new Brake() ); 
            suvCar.applyBrake();    // This will invoke class "Brake" 
        }
    }

This gives greater flexibility in design and is in harmony with the __[Open/closed principle](https://en.wikipedia.org/wiki/Open/closed_principle)__ (OCP) that states that __classes should be open for extension but closed for modification__.
]]></content>
  </entry>
  
</feed>

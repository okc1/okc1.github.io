<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: Design | Woodstock Blog]]></title>
  <link href="http://okc1.github.io/blog/categories/design/atom.xml" rel="self"/>
  <link href="http://okc1.github.io/"/>
  <updated>2016-06-13T01:11:53-07:00</updated>
  <id>http://okc1.github.io/</id>
  <author>
    <name><![CDATA[Kevin Durant]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[[Design] User Registry Table Design]]></title>
    <link href="http://okc1.github.io/blog/2016/05/08/user-registry-table-design/"/>
    <updated>2016-05-08T00:00:00-07:00</updated>
    <id>http://okc1.github.io/blog/2016/05/08/user-registry-table-design</id>
    <content type="html"><![CDATA[<h1>First word</h1>

<p>Designing a system like twitter, facebook or airbnb, first step is often <strong>User Registry</strong>.</p>

<p>The tables, <strong>we must use RDBMS</strong>, as it&rsquo;s more reliable.</p>

<h2>Table design</h2>

<p><strong>Friendship table</strong> is important:</p>

<p>{% img middle /assets/images/design-user-registry-tables.png %}</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Design] Technical Challenges of Writing a Web Crawler]]></title>
    <link href="http://okc1.github.io/blog/2015/11/22/challenges-writing-crawler/"/>
    <updated>2015-11-22T00:00:00-08:00</updated>
    <id>http://okc1.github.io/blog/2015/11/22/challenges-writing-crawler</id>
    <content type="html"><![CDATA[<h1>1. Choose a framework</h1>

<p>Assuming we use Python to do this.</p>

<h2>plain python?</h2>

<p>We can write a simple Python crawler with the code below:</p>

<pre><code>import re, urllib

textfile = file('depth_1.txt','wt')
print "Enter the URL you wish to crawl.."
print 'Usage  - "http://phocks.org/stumble/creepy/" &lt;-- With the double quotes'
myurl = input("@&gt; ")
for i in re.findall('''href=["'](.[^"']+)["']''', urllib.urlopen(myurl).read(), re.I):
    print i  
    for ee in re.findall('''href=["'](.[^"']+)["']''', urllib.urlopen(i).read(), re.I):
        print ee
        textfile.write(ee+'\n')
textfile.close()
</code></pre>

<h2>Scrapy?</h2>

<ol>
<li>You only define the rules, Scrapy do the rest</li>
<li>easily plugin extensions</li>
<li>portable + python runtime.</li>
</ol>


<h3>Why Scrapy</h3>

<blockquote><p><a href="https://www.quora.com/What-are-the-advantages-of-Scrapy-compared-to-Beautiful-Soup">scrapy has the tools to manage every stage of a web crawl</a>, just to name a few:</p>

<ol>
<li><p>Requests manager - in charge of downloading pages all concurrently behind the scenes! You won&rsquo;t need to invest a lot of time in concurrent architecture.</p></li>
<li><p>Selectors -  parse the html document (eg. XPath)</p></li>
<li><p>Pipelines - after you retrieve the data, there&rsquo;s a bunch of functions to modify the data.</p></li>
</ol>
</blockquote>

<p>Following the spirit of other don’t repeat yourself frameworks, such as Django:</p>

<blockquote><p><a href="https://en.wikipedia.org/wiki/Scrapy">it makes it easier to build and scale large crawling projects</a> by allowing developers to re-use their code.</p></blockquote>

<p>For more, read <a href="http://doc.scrapy.org/en/latest/topics/architecture.html">Scrapy Architecture </a>.</p>

<p>{% img middle /assets/images/scrapy_architecture.png %}</p>

<ol>
<li><p>Scrapy Engine</p>

<p> control data flow</p></li>
<li><p>Scheduler</p>

<p> receives requests from the engine and enqueues them for feeding them later</p></li>
<li><p>Downloader</p></li>
<li><p>Spiders</p></li>
<li><p>Item Pipeline</p></li>
<li><p>Downloader middlewares</p>

<p> specific hooks that sit between the Engine and the Downloader and process requests</p></li>
<li><p>Spider middlewares</p>

<p> specific hooks that sit between the Engine and the Spiders and are able to process spider input (responses) and output (items and requests).</p></li>
</ol>


<h1>2. Schedule a Scrapy job</h1>

<p>APScheduler? (todo)</p>

<p>add/remove jobs</p>

<h1>3. Choose a DB</h1>

<p>I chose NoSQL/MongoDB. <a href="http://stackoverflow.com/a/11980154">But why</a>?</p>

<ol>
<li><p>there&rsquo;s only a few tables with few columns</p></li>
<li><p>no overly complex associations between nodes</p></li>
<li><p>huge amount of time-based data</p></li>
<li><p>scaling requirements: MongoDB better horizontal scaling</p></li>
<li><p>different field names: dynamical storage</p></li>
</ol>


<h1>4. Technical Difficulty?</h1>

<h2>4.1 differrent way to crawl.</h2>

<p>We need to check AJAX response sometime and study each website&rsquo;s API.</p>

<p>Some site would <strong>close certain APIs</strong> if they found out too many queries requests.</p>

<h2>4.2 Difficulty navigating pages</h2>

<p>Study their URL structure.</p>

<p>eg.</p>

<pre><code>www.abc.com/index.html?page=milk&amp;start_index=0
</code></pre>

<p>Just play with the url params!</p>

<h2>4.3 What is key?</h2>

<p>I defined extra column only to store keys (combine a few key columns, and convert to lower-case).</p>

<p>We can search using <strong>regex</strong> though, but:</p>

<blockquote><p><a href="http://stackoverflow.com/a/7880894">Mongo (current version 2.0.0) doesn&rsquo;t allow</a> case-insensitive searches against indexed fields. For non-indexed fields, the regex search should be fine.</p></blockquote>

<p>How to go about it:</p>

<blockquote><p><a href="http://stackoverflow.com/a/4441412">searching with regex&rsquo;s case insensitive</a> means that mongodb cannot search by index, so queries against <strong>large datasets can take a long time</strong>.</p>

<p>Even with small datasets, it&rsquo;s not very efficient&hellip; which could become an issue if you are trying to achieve scale.</p>

<p>As an alternative, you can store an uppercase copy and search against that&hellip;</p>

<p>If your field is large, such as a message body, duplicating data is probably not a good option. I believe using <strong>an extraneous indexer like Apache Lucene</strong> is the best option in that case.</p></blockquote>

<h2>4.4 A lot bad data</h2>

<ol>
<li><p>write a sophisticated pipeline()</p></li>
<li><p>try not let bad data reach pipeline() - <strong>better</strong></p></li>
</ol>


<p>Make your spider better!</p>

<h2>4.5 NLP: brand names</h2>

<p>how? (todo)</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Design] How to Generate Maze]]></title>
    <link href="http://okc1.github.io/blog/2015/11/21/generate-maze/"/>
    <updated>2015-11-21T00:00:00-08:00</updated>
    <id>http://okc1.github.io/blog/2015/11/21/generate-maze</id>
    <content type="html"><![CDATA[<h1>Question</h1>

<p><a href="http://www.glassdoor.com/Interview/Design-a-2D-dungeon-crawling-game-It-must-allow-for-various-items-in-the-maze-walls-objects-and-computer-controlled-c-QTN_57.htm">link</a></p>

<blockquote><p>Design a 2D dungeon crawling game. It must allow for   various items in the maze - walls, objects, and computer-controlled characters.</p></blockquote>

<h1>Part 1: API design</h1>

<p>Serialize:</p>

<blockquote><p><a href="http://qr.ae/RbRhHv">if a certain cell has a wall</a> to the North and West but not to the South or East, it would be represented as 1001, or 9&hellip; (e.g., &ldquo;9,6,11,12\n3,10,10,4\n13,9,12,5\n3,6,1,6&rdquo; in a 4x4 maze)</p></blockquote>

<p>Design API~</p>

<h1>Part 2: Algorithm</h1>

<h2>Depth-first search</h2>

<p>This is most common and <a href="https://en.wikipedia.org/wiki/Maze_generation_algorithm#Depth-first_search">one of the simplest ways to generate a maze using a computer</a>. It&rsquo;s commonly implemented using <strong><a href="https://en.wikipedia.org/wiki/Maze_generation_algorithm#Recursive_backtracker">Recursive backtrack</a></strong>.</p>

<ol>
<li><p>from a random cell, select a random neighbour that hasn&rsquo;t been visited.</p></li>
<li><p>removes the &lsquo;wall&rsquo; and adds the new cell to a stack.</p></li>
<li><p>a cell with no unvisited neighbours is considered <strong>dead-end</strong>.</p></li>
<li><p>When at a dead-end it backtracks through the path until it reaches a cell with unvisited neighbours, continuing from there.</p></li>
<li><p>until every cell has been visited, the computer would backtrack all the way to the beginning cell.</p></li>
<li><p>Entire maze space is guaranted a complete visit.</p></li>
</ol>


<h3>side note</h3>

<blockquote><p>To add difficulty and a fun factor to the DFS, you can <strong>influence the likelihood of which neighbor you should visit</strong>, instead of completely random.</p>

<p>By making it more likely to visit neighbors to your sides, you can have a more horizontal maze generation.</p></blockquote>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Design] Strategy Design Pattern]]></title>
    <link href="http://okc1.github.io/blog/2015/11/18/strategy-pattern/"/>
    <updated>2015-11-18T00:00:00-08:00</updated>
    <id>http://okc1.github.io/blog/2015/11/18/strategy-pattern</id>
    <content type="html"><![CDATA[<h1>Overview</h1>

<p><strong><a href="https://en.wikipedia.org/wiki/Strategy_pattern">Strategy pattern</a></strong> (also known as the policy pattern) is a design pattern that <strong>enables an algorithm&rsquo;s behavior to be selected</strong> at runtime.</p>

<p>For instance, a class that performs <strong>validation on incoming data</strong> may use a strategy pattern to select a validation algorithm <strong>based on the type of data</strong>, the source of the data, user choice&hellip; These factors are not known <strong>until run-time</strong>&hellip;</p>

<h2>A car example</h2>

<p>Since accelerate and brake behaviors change frequently between models, <strong>a common approach is to implement these behaviors in subclasses</strong>. This approach has significant drawbacks: accelerate and brake behaviors <strong>must be declared in each new Car model</strong>.</p>

<p>{% img middle /assets/images/600px-StrategyPattern_IBrakeBehavior.png %}</p>

<p>The strategy pattern uses <strong>composition</strong> instead of inheritance. This allows:</p>

<ol>
<li><p>better <strong>decoupling between the behavior</strong> and the class that uses it. (i.e. behavior can be changed without breaking the classes that use it)</p></li>
<li><p>classes can switch between behaviors by changing the specific implementation used without requiring any significant code changes.</p></li>
</ol>


<p>Code:</p>

<pre><code>/* Encapsulated family of Algorithms 
 * Interface and its implementations
 */
public interface IBrakeBehavior {
    public void brake(); 
}

public class BrakeWithABS implements IBrakeBehavior {
    public void brake() {
        System.out.println("Brake with ABS applied");
    }
}

public class Brake implements IBrakeBehavior {
    public void brake() {
        System.out.println("Simple Brake applied");
    }
}


/* Client which can use the algorithms above interchangeably */
public abstract class Car {
    protected IBrakeBehavior brakeBehavior;

    public void applyBrake() {
        brakeBehavior.brake();
    }

    public void setBrakeBehavior(IBrakeBehavior brakeType) {
        this.brakeBehavior = brakeType;
    }
}

/* Client 1 uses one algorithm (Brake) in the constructor */
public class Sedan extends Car {
    public Sedan() {
        this.brakeBehavior = new Brake();
    }
}

/* Client 2 uses another algorithm (BrakeWithABS) in the constructor */
public class SUV extends Car {
    public SUV() {
        this.brakeBehavior = new BrakeWithABS();
    }
}


/* Using the Car Example */
public class CarExample {
    public static void main(String[] args) {
        Car sedanCar = new Sedan();
        sedanCar.applyBrake();  // This will invoke class "Brake"

        Car suvCar = new SUV(); 
        suvCar.applyBrake();    // This will invoke class "BrakeWithABS"

        // set brake behavior dynamically
        suvCar.setBrakeBehavior( new Brake() ); 
        suvCar.applyBrake();    // This will invoke class "Brake" 
    }
}
</code></pre>

<p>This gives greater flexibility in design and is in harmony with the <strong><a href="https://en.wikipedia.org/wiki/Open/closed_principle">Open/closed principle</a></strong> (OCP) that states that <strong>classes should be open for extension but closed for modification</strong>.</p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[[Design] Facebook Photo Storage]]></title>
    <link href="http://okc1.github.io/blog/2015/09/02/Facebook-photo-storage/"/>
    <updated>2015-09-02T00:00:00-07:00</updated>
    <id>http://okc1.github.io/blog/2015/09/02/Facebook-photo-storage</id>
    <content type="html"><![CDATA[<h1>Stats</h1>

<p>Facebook has 1.5 billion monthly active users, 970 million daily active users <a href="http://newsroom.fb.com/company-info/">as of June 2015</a>.</p>

<p>{% img middle /assets/images/facebook-user-count.png %}</p>

<p>image from <a href="http://www.statista.com/statistics/264810/number-of-monthly-active-facebook-users-worldwide/">statista.com</a>.</p>

<p>In 2009, Facebook stores 15 billion photos for the user, which grows at 220 million per week, and 550,000 per second at peak.</p>

<p>It&rsquo;s 2015 now, you might want to mulitply these numbers by 3~6.</p>

<p>I have roughly estimated the statistics of Facebook users, Facebook photos and growth rate, just to give you an idea of the size of data Facebook has got:</p>

<blockquote><p>Total user: 1.5b</p>

<p>Total photoes: 150b, which is 100 photo/user</p>

<p>Each photo got 4 different sizes, so 600b photos are stored.</p>

<p>New photo per day: 500m</p>

<p>New photo per second: 6,000</p>

<p>Peak incoming photo per second: 3m</p></blockquote>

<h1>Old architecture</h1>

<p>3 tiers design:</p>

<ol>
<li><p><strong>Upload tier</strong> receives users’ photo uploads, scales the original images and saves them on the NFS storage tier.</p></li>
<li><p><strong>Photo serving tier</strong> receives HTTP requests for photo images and serves them from the NFS storage tier.</p></li>
<li><p><strong>NFS storage tier</strong> built on top of commercial storage appliances.</p></li>
</ol>


<blockquote><p><strong><a href="https://en.wikipedia.org/wiki/Network_File_System">Network File System</a></strong> (NFS) is a distributed file system protocol originally developed by Sun Microsystems in 1984, allowing a user on a client computer to access files over a network much like local storage is accessed.</p></blockquote>

<h2>Problem</h2>

<ol>
<li><p>there is an <strong>enormous amount of metadata</strong></p>

<p> &hellip; so much that is <a href="https://code.facebook.com/posts/685565858139515/needle-in-a-haystack-efficient-storage-of-billions-of-photos/">exceeds the caching abilities of the NFS storage tier</a>, <strong> resulting in multiple I/O operations</strong> per photo upload or read request</p></li>
</ol>


<h2>Solution</h2>

<ol>
<li><p>relies heavily on CDNs to serve photos.</p></li>
<li><p>Cachr: a caching server tier caching Facebook &ldquo;profile&rdquo; images.</p></li>
<li><p>NFS file handle cache - deployed on the photo serving tier eliminates some of the NFS storage tier metadata overhead</p></li>
</ol>


<h1>Haystack Photo Infrastructure</h1>

<p>The new photo infrastructure merges the <strong>photo serving</strong> and <strong>storage tier</strong> into one physical tier. It implements <strong>a HTTP based photo server</strong> which stores photos in a generic object store called Haystack.</p>

<p>Goal: eliminate any unnecessary metadata overhead for photo read operations, so that each read I/O operation was only reading actual photo data</p>

<p>5 main functional layers:</p>

<ol>
<li><p>HTTP server</p></li>
<li><p>Photo Store</p></li>
<li><p>Haystack Object Store</p></li>
<li><p>Filesystem</p></li>
<li><p>Storage</p></li>
</ol>


<h2>Storage</h2>

<p>The commodity machine HW typically is 2x quadcore CPU + 32GB RAM + 512MB NV-RAM cache + 12TB SATA drives.</p>

<blockquote><p><a href="https://en.wikipedia.org/wiki/Non-volatile_random-access_memory">Non-volatile random-access memory</a> (NVRAM) is random-access memory that retains its information when power is turned off (non-volatile).</p>

<p>This is in contrast to dynamic random-access memory (DRAM) and static random-access memory (SRAM)</p></blockquote>

<p>So each <strong>storage blade</strong> is around 10TB. Configured as <strong>RAID-6</strong> partition.</p>

<blockquote><p><a href="http://searchstorage.techtarget.com/definition/RAID-6-redundant-array-of-independent-disks">RAID 6</a>, also known as double-parity RAID, uses two parity stripes on each disk. It allows for two disk failures within the RAID set before any data is lost.</p></blockquote>

<p>Pros:</p>

<ol>
<li>adequate redundancy</li>
<li>excellent read performance</li>
<li>low storage cost down</li>
</ol>


<p>Cons:</p>

<p><strong>The poor write performance</strong> is partially mitigated by the <strong>RAID controller NVRAM write-back cache</strong>. Since the reads are mostly random, the NVRAM cache is fully reserved for writes.</p>

<p><strong>The disk caches are disabled</strong> in order to guarantee data consistency in the event of a crash or a power loss.</p>

<h2>Filesystem</h2>

<h3>How does filesystem read pictures?</h3>

<p>Photo read requests result in <strong>read() system calls</strong> at known offsets in these files.</p>

<p>Each file in the filesystem is represented by a structure called an inode which contains a block map that maps the logical file offset to the physical block offset on the physical volume.</p>

<p>For large files, the block map can be quite large.</p>

<h3>Problem</h3>

<p><strong>Block based filesystems</strong> maintain mappings for <strong>each logical block</strong>, and for large files, this information will not typically fit into the cached inode and is stored in indirect address blocks instead, which must be traversed in order to read the data for a file.</p>

<p>There can be several layers of indirection, so a single read could result in <strong>several I/Os</strong> (if not cached).</p>

<h3>Solution</h3>

<p><strong>Extent based filesystems</strong> maintain mappings only for contiguous ranges of blocks (extents). A block map for a contiguous large file could consist of only one extent which would fit in the inode itself.</p>

<blockquote><p><a href="https://goo.gl/uQA35V">An extent</a> is a contiguous area of storage reserved for a file in a file system, represented as a range. A file can consist of zero or more extents; <strong>one file fragment requires one extent</strong>. The direct benefit is in storing each range compactly as two numbers, instead of canonically storing every block number in the range.</p>

<p>Extent-based file systems can also <strong>eliminate most of the metadata overhead of large files</strong> that would traditionally be taken up by the block allocation tree&hellip; saving on storage space is pretty slight, but&hellip; <strong>the reduction in metadata is significant and reduces exposure to file system corruption</strong> as one bad sector in the <em>block allocation tree</em> causes much greater data loss than one bad sector in stored data.</p></blockquote>

<h3>Problem of Extent-based file systems</h3>

<p>However, if the file is severely fragmented and its blocks are not contiguous on the underlying volume, its block map can grow large as well.</p>

<h3>The solution</h3>

<p>Fragmentation can be mitigated by <strong>aggressively allocating a large chunk of space</strong> whenever growing the physical file.</p>

<p>Currently, the filesystem of choice is XFS, an extent based filesystem providing efficient file preallocation.</p>

<blockquote><p><strong><a href="https://en.wikipedia.org/wiki/XFS">XFS</a></strong> is a high-performance 64-bit journaling file system created by Silicon Graphics, Inc (SGI) in 1993.</p>

<p>&hellip;was ported to the Linux kernel in 2001. As of June 2014, XFS is supported by most Linux distributions, some of which use it as the default file system.</p>

<p>XFS enables <strong>extreme scalability of I/O threads</strong>, file system bandwidth, and size of files and of the file system itself when spanning multiple physical storage devices.</p>

<p>Space allocation is performed via extents with <strong>data structures stored in B+ trees</strong>, improving the overall performance of the file system, especially when handling large files.</p>

<p><strong>Delayed allocation</strong> assists in the prevention of file system fragmentation; <strong>online defragmentation</strong> is also supported. A feature unique to XFS is the <strong>pre-allocation of I/O bandwidth</strong> at a pre-determined rate, which is suitable for many real-time applications.</p></blockquote>

<h2>Haystack</h2>

<p>Haystack is a simple <strong>log structured (append-only) object store</strong>. Many images store in one Haystack. Typically, <a href="http://jishu.zol.com.cn/17742.html">Haystack Store is 100GB size</a>.</p>

<p>A Haystack consists of two files:</p>

<ol>
<li><strong>haystack store file</strong> (containing the needles)

<ol>
<li>superblock</li>
<li>needles (1 needle is 1 image)</li>
</ol>
</li>
<li><strong>an index file</strong></li>
</ol>


<h3>1. haystack store file</h3>

<p>{% img middle /assets/images/851582_1409519009260319_311676661_n.jpg %}</p>

<ol>
<li><p>The first 8KB of the haystack store is occupied by the <strong>superblock</strong>.</p></li>
<li><p>following the superblock are <strong>needles</strong></p>

<p> <strong>Needles represents the stored objects</strong>. Needle consisting of a header, the data, and a footer:</p>

<p> {% img middle /assets/images/851578_319395058204993_541487263_n.jpg %}</p>

<p> A needle is uniquely identified by its \&lt;Offset, Key, Alternate Key, Cookie> tuple.</p>

<p> Haystack doesn’t put any restriction on the values of the keys, and there can be needles with duplicate keys.</p></li>
</ol>


<h3>2. Index File</h3>

<p><strong>Needle</strong> consisting of a header, the data, and a footer:</p>

<p>{% img middle /assets/images/851582_1374324519464800_699636937_n.jpg %}</p>

<p>The index file provides the minimal metadata required to locate a particular needle in the haystack store file.</p>

<p>{% img middle /assets/images/851582_314454922033518_1196942525_n.jpg %}</p>

<p>The index file is not critical, as it can be rebuilt from the haystack store file if required.</p>

<p>The main purpose of the index: quick loading of the needle metadata into memory without traversing the larger Haystack store file, since the index is usually less than 1% the size of the store file.</p>

<h3>Summary</h3>

<p>All needles are stored in Haystack store file, and their location information is stored in Index File.</p>

<p>What is Needle? Needles represents the stored objects (1 needle - 1 image).</p>

<h2>Haystack Operations</h2>

<ol>
<li><p><strong>write</strong></p>

<p> append new needle to haystack store file.</p>

<p> later, corresponding index records are <strong>updated async</strong>.</p>

<p> In case of failure, any partial needles are discarded, and fix index from the end of the haystack.</p>

<p> You can&rsquo;t overwrite any needle, but you can insert using same key. Later, the needle <strong>with dup keys but largest offset</strong> became the most recent one.</p></li>
<li><p><strong>read</strong></p>

<p> <strong>parameters passed in</strong>: offset, key, alt key, cookie, data size</p>

<p> if key, alt key and cookie matches, and checksum correct and needle is not marked as deleted, return.</p></li>
<li><p><strong>delete</strong></p>

<p> marks needle as deleted (set 1 bit), but index is not modified.</p>

<p> the deleted space is not reclaimed unless <strong>compact the haystack</strong></p></li>
</ol>


<h2>Photo Store Server</h2>

<p>Photo Store Server is responsible for accepting HTTP requests and translating them to the corresponding Haystack store operations.</p>

<p>In order to minimize the number of I/Os required to retrieve photos, the server keeps an <strong>in-memory index of all photo offsets</strong>.</p>

<p>At startup, <strong>the (photo) server reads the haystack index file and populates the in-memory index</strong>. The in-memory index is different from the index file in Haystack, as it stores lesser information, like this:</p>

<p>{% img middle /assets/images/851584_503528913060377_1268325854_n.jpg %}</p>

<ol>
<li><p><strong>Photo Store Write/Modify Operation</strong></p>

<ol>
<li>writes photos to the haystack</li>
<li>updates the in-memory index</li>
</ol>


<p> if there are duplicate images, the one stored at a larger offset is valid.</p></li>
<li><p><strong>Photo Store Read Operation</strong></p>

<p> passed in:</p>

<ol>
<li>haystack id</li>
<li>a photo key,</li>
<li>size</li>
<li>cookie</li>
</ol>


<p> The server performs a lookup in the in-memory index based on the photo key and retrieves the offset of the needle containing the requested image.</p>

<p> Since haystack delete operation doesn’t update the haystack index file record. Therefore a freshly populated in-memory index can contain stale entries for the previously deleted photos. Read such photo will fail and the in-memory index is updated to 0.</p></li>
<li><p><strong>Photo Store Delete Operation</strong></p>

<p> After calling the haystack delete operation, the in-memory index is updated to &lsquo;offset = zero&rsquo;</p></li>
<li><p><strong>Compaction</strong></p>

<p> Compaction is an online operation which reclaims the space used by the deleted and duplicate needles.</p>

<p> creates a new haystack</p></li>
<li><p><strong>HTTP Server</strong></p>

<p> evhttp server</p>

<p> multiple threads, with each serving a single HTTP request. Workload is mostly I/O bound, thus the performance of the HTTP server is not critical.</p></li>
</ol>


<h1>Summary</h1>

<p>Storing photos as needles in the haystack <strong>eliminates the metadata overhead</strong> by aggregating hundreds of thousands of images in a single haystack store file.</p>

<p>This keeps the metadata overhead very small and allows us to <strong>store each needle’s location in the store file in an in-memory index</strong>.</p>

<p>This allows retrieval of an image’s data in <strong>a minimal number of I/O operations</strong>, eliminating all unnecessary metadata overhead.</p>

<p>Ref: <a href="https://code.facebook.com/posts/685565858139515/needle-in-a-haystack-efficient-storage-of-billions-of-photos/">https://code.facebook.com/posts/685565858139515/needle-in-a-haystack-efficient-storage-of-billions-of-photos/</a></p>
]]></content>
  </entry>
  
</feed>
